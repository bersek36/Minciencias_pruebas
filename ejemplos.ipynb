{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\r\n",
    "\r\n",
    "import numpy as np \r\n",
    "import nibabel as nib\r\n",
    "import tensorflow as tf\r\n",
    "\r\n",
    "from preprocess.get_subvolume import get_training_sub_volumes\r\n",
    "from unet3d import *\r\n",
    "\r\n",
    "np.set_printoptions(precision=2, suppress=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "PARENT_DIR = os.getcwd()\r\n",
    "img = os.path.join(PARENT_DIR, \"A00028185\\sub-A00028185_ses-NFB3_T1w.nii.gz\")\r\n",
    "img_mask = os.path.join(PARENT_DIR, \"A00028185\\sub-A00028185_ses-NFB3_T1w_brainmask.nii.gz\")\r\n",
    "img = nib.load(img)\r\n",
    "img_mask = nib.load(img_mask)\r\n",
    "img.affine"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[ -0.  ,  -0.  ,   1.  , -98.2 ],\n",
       "       [ -1.  ,  -0.  ,  -0.  , 155.19],\n",
       "       [  0.  ,  -1.  ,   0.  ,  99.04],\n",
       "       [  0.  ,   0.  ,   0.  ,   1.  ]])"
      ]
     },
     "metadata": {},
     "execution_count": 2
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "image = img.get_fdata()\r\n",
    "image_mask = img_mask.get_fdata()\r\n",
    "image[:,:,:].shape"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(256, 256, 192)"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "SAVE_PATH = \"train_sub_volumes\"\r\n",
    "PATH_SUBVOLUME = \"images\"\r\n",
    "PATH_SUBMASK = \"masks\"\r\n",
    "SAVE_PATH_SUBVOLUME = os.path.join(PARENT_DIR, SAVE_PATH, PATH_SUBVOLUME, \"A00028185\")\r\n",
    "SAVE_PATH_SUBMASK = os.path.join(PARENT_DIR, SAVE_PATH, PATH_SUBMASK, \"A00028185\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# get_training_sub_volumes(image, img.affine, image_mask, img_mask.affine, \r\n",
    "#                                 SAVE_PATH_SUBVOLUME, SAVE_PATH_SUBMASK, \r\n",
    "#                                 classes=1, \r\n",
    "#                                 orig_x = 256, orig_y = 256, orig_z = 192, \r\n",
    "#                                 output_x = 128, output_y = 128, output_z = 16,\r\n",
    "#                                 stride_x = 40, stride_y = 40, stride_z = 8,\r\n",
    "#                                 background_threshold=0.0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "feat_dir = 'train_sub_volumes'\r\n",
    "\r\n",
    "train_ids = next(os.walk(feat_dir))[1] # [2]: files; [1]: directories\r\n",
    "train_ids"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['images', 'masks']"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "images=[]\r\n",
    "images_mask=[]\r\n",
    "for subvol in sorted(os.listdir(os.path.join(feat_dir, train_ids[0]))):\r\n",
    "    for item in os.listdir(os.path.join(feat_dir, train_ids[0],subvol)):\r\n",
    "        images.append(os.path.join(feat_dir,train_ids[0],subvol,item))\r\n",
    "\r\n",
    "for subvol in sorted(os.listdir(os.path.join(feat_dir, train_ids[1]))):\r\n",
    "    for item in os.listdir(os.path.join(feat_dir, train_ids[1],subvol)):\r\n",
    "        images_mask.append(os.path.join(feat_dir,train_ids[1],subvol,item))\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "len(images_mask)"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "143"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "source": [
    "def load_image(file, label):\r\n",
    "    nifti = np.asarray(nib.load(file.numpy().decode('utf-8')).get_fdata()).astype(np.int16)\r\n",
    "    label = np.asarray(nib.load(label.numpy().decode('utf-8')).get_fdata()).astype(np.int16)\r\n",
    "    return nifti, label\r\n",
    "\r\n",
    "\r\n",
    "@tf.autograph.experimental.do_not_convert\r\n",
    "def load_image_wrapper(file, label):\r\n",
    "    image, label = tf.py_function(load_image, [file, label], [tf.int16, tf.int16])\r\n",
    "    image.set_shape(tf.TensorShape([128, 128, 16]))\r\n",
    "    label.set_shape(tf.TensorShape([128, 128, 16]))\r\n",
    "    return image, label\r\n",
    "\r\n",
    "dataset = tf.data.Dataset.from_tensor_slices((images, images_mask))\r\n",
    "dataset = dataset.map(load_image_wrapper, num_parallel_calls=32)\r\n",
    "dataset = dataset.batch(32, drop_remainder=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "source": [
    "model_unet=unet_3D(128, 128, 16)#(60,160,16)\r\n",
    "model_unet.compile(optimizer='adam', loss = 'binary_crossentropy', #\"categorical_crossentropy\", \r\n",
    "                    metrics = ['accuracy', dice_coeff]) "
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(None, 128, 128, 16, 1) \n",
      "\n",
      "(None, 128, 128, 16, 16)\n",
      "(None, 128, 128, 16, 16)\n",
      "(None, 64, 64, 8, 16)\n",
      "(None, 64, 64, 8, 32)\n",
      "(None, 64, 64, 8, 32)\n",
      "(None, 32, 32, 4, 32)\n",
      "(None, 32, 32, 4, 64)\n",
      "(None, 32, 32, 4, 64)\n",
      "(None, 16, 16, 2, 64)\n",
      "(None, 16, 16, 2, 128)\n",
      "(None, 16, 16, 2, 128)\n",
      "(None, 8, 8, 1, 128) \n",
      "\n",
      "(None, 8, 8, 1, 256)\n",
      "(None, 8, 8, 1, 256) \n",
      "\n",
      "(None, 16, 16, 2, 128)\n",
      "(None, 16, 16, 2, 256)\n",
      "(None, 16, 16, 2, 128)\n",
      "(None, 16, 16, 2, 128)\n",
      "(None, 32, 32, 4, 64)\n",
      "(None, 32, 32, 4, 128)\n",
      "(None, 32, 32, 4, 64)\n",
      "(None, 32, 32, 4, 64)\n",
      "(None, 64, 64, 8, 32)\n",
      "(None, 64, 64, 8, 64)\n",
      "(None, 64, 64, 8, 32)\n",
      "(None, 64, 64, 8, 32)\n",
      "(None, 128, 128, 16, 16)\n",
      "(None, 128, 128, 16, 32)\n",
      "(None, 128, 128, 16, 16)\n",
      "(None, 128, 128, 16, 16) \n",
      "\n",
      "(None, 128, 128, 16, 1)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "callbacks = ModelCheckpoint(PARENT_DIR, \r\n",
    "                            monitor='val_dice_coeff', #accuracy', # val_acc\r\n",
    "                            verbose=1, \r\n",
    "                            mode='max',\r\n",
    "                            save_best_only=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "history = model_unet.fit(dataset,\r\n",
    "                         epochs=100,\r\n",
    "                         callbacks=[callbacks]) #Guardar la mejor epoca para validaciÃ³n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1/100\n",
      "4/4 [==============================] - 449s 110s/step - loss: 1.7813 - accuracy: 0.6824 - dice_coeff: 0.2994\n",
      "WARNING:tensorflow:Can save best model only with val_dice_coeff available, skipping.\n",
      "Epoch 2/100\n",
      "4/4 [==============================] - 435s 109s/step - loss: 0.7021 - accuracy: 0.7815 - dice_coeff: 0.4009\n",
      "WARNING:tensorflow:Can save best model only with val_dice_coeff available, skipping.\n",
      "Epoch 3/100\n",
      "4/4 [==============================] - 463s 116s/step - loss: 0.8100 - accuracy: 0.8215 - dice_coeff: 0.3700\n",
      "WARNING:tensorflow:Can save best model only with val_dice_coeff available, skipping.\n",
      "Epoch 4/100\n",
      "4/4 [==============================] - 453s 112s/step - loss: 0.5748 - accuracy: 0.7783 - dice_coeff: 0.4963\n",
      "WARNING:tensorflow:Can save best model only with val_dice_coeff available, skipping.\n",
      "Epoch 5/100\n",
      "4/4 [==============================] - 450s 111s/step - loss: 0.4929 - accuracy: 0.7917 - dice_coeff: 0.3135\n",
      "WARNING:tensorflow:Can save best model only with val_dice_coeff available, skipping.\n",
      "Epoch 6/100\n",
      "4/4 [==============================] - 442s 111s/step - loss: 0.4355 - accuracy: 0.7958 - dice_coeff: 0.2826\n",
      "WARNING:tensorflow:Can save best model only with val_dice_coeff available, skipping.\n",
      "Epoch 7/100\n",
      "4/4 [==============================] - 454s 114s/step - loss: 0.4718 - accuracy: 0.7654 - dice_coeff: 0.3480\n",
      "WARNING:tensorflow:Can save best model only with val_dice_coeff available, skipping.\n",
      "Epoch 8/100\n",
      "4/4 [==============================] - 463s 115s/step - loss: 0.4825 - accuracy: 0.7604 - dice_coeff: 0.3336\n",
      "WARNING:tensorflow:Can save best model only with val_dice_coeff available, skipping.\n",
      "Epoch 9/100\n",
      "4/4 [==============================] - 433s 108s/step - loss: 0.4536 - accuracy: 0.7728 - dice_coeff: 0.3513\n",
      "WARNING:tensorflow:Can save best model only with val_dice_coeff available, skipping.\n",
      "Epoch 10/100\n",
      "4/4 [==============================] - 431s 108s/step - loss: 0.4085 - accuracy: 0.7942 - dice_coeff: 0.4097\n",
      "WARNING:tensorflow:Can save best model only with val_dice_coeff available, skipping.\n",
      "Epoch 11/100\n",
      "4/4 [==============================] - 424s 106s/step - loss: 0.3399 - accuracy: 0.8262 - dice_coeff: 0.4915\n",
      "WARNING:tensorflow:Can save best model only with val_dice_coeff available, skipping.\n",
      "Epoch 12/100\n",
      "4/4 [==============================] - 429s 107s/step - loss: 0.3177 - accuracy: 0.8226 - dice_coeff: 0.5393\n",
      "WARNING:tensorflow:Can save best model only with val_dice_coeff available, skipping.\n",
      "Epoch 13/100\n",
      "4/4 [==============================] - 437s 109s/step - loss: 0.2848 - accuracy: 0.8437 - dice_coeff: 0.5543\n",
      "WARNING:tensorflow:Can save best model only with val_dice_coeff available, skipping.\n",
      "Epoch 14/100\n",
      "2/4 [==============>...............] - ETA: 3:40 - loss: 0.3002 - accuracy: 0.8461 - dice_coeff: 0.5240"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "source": [
    "import matplotlib.pyplot as plt\r\n",
    "plt.imshow(images_mask[1][:,:,15], cmap=\"gray\")"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19257ea7280>"
      ]
     },
     "metadata": {},
     "execution_count": 153
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_slice(prueba_img, test_mask_img, predictions_img)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}